================================================================================
 ENCODER vs DECODER INJECTIVITY COMPARISON
 Based on: 'Language Models are Injective and Hence Invertible'
================================================================================

ðŸŽ¯ RESEARCH QUESTION
--------------------------------------------------------------------------------
Does the injectivity property proven for decoder models (GPT-2) also hold
for encoder models (BERT)? Can prompts be reconstructed from both architectures?

ðŸ—ï¸ ARCHITECTURAL DIFFERENCES
--------------------------------------------------------------------------------
GPT-2 (DECODER):
  â€¢ Unidirectional (left-to-right) attention
  â€¢ Autoregressive generation
  â€¢ Predicts next token
  â€¢ Natural 'last token' representation

BERT (ENCODER):
  â€¢ Bidirectional attention
  â€¢ Processes entire sequence
  â€¢ Masked language modeling
  â€¢ Uses [CLS] token or pooling

ðŸ“Š QUANTITATIVE COMPARISON - QUERIES
================================================================================
Metric               GPT-2 (Decoder)           BERT (Encoder)           
--------------------------------------------------------------------------------
Min Distance         11.320743                 2.466962                 
Mean Distance        25.565545                 5.124040                 
Max Distance         37.293880                 7.359100                 
Std Deviation        4.816464                  0.808114                 
Injective?           âœ“ YES                     âœ“ YES                    

ðŸ“Š QUANTITATIVE COMPARISON - ANSWERS
================================================================================
Metric               GPT-2 (Decoder)           BERT (Encoder)           
--------------------------------------------------------------------------------
Min Distance         14.666336                 2.551789                 
Mean Distance        22.178159                 4.994930                 
Max Distance         30.160366                 6.702584                 
Std Deviation        3.556958                  0.708754                 
Injective?           âœ“ YES                     âœ“ YES                    

ðŸ” KEY FINDINGS
================================================================================
1. BOTH ARCHITECTURES EXHIBIT INJECTIVITY
   âœ“ All pairwise distances > 0 for both GPT-2 and BERT
   âœ“ Each unique input maps to a unique hidden state
   âœ“ Injectivity is NOT architecture-specific

2. SEPARATION STRENGTH
   GPT-2 shows stronger separation (avg min: 12.993539)
   BERT has smaller margins (avg min: 2.509375)
   This may be due to BERT's bidirectional context compression

3. IMPLICATIONS FOR PROMPT RECONSTRUCTION
   âœ“ SIPIT algorithm (from paper) designed for decoders
   âœ“ Encoder reconstruction would need different approach
   âœ“ Both architectures are theoretically invertible
   âœ“ Practical inversion may differ in complexity

ðŸŽ¯ CONCLUSIONS
================================================================================
MAIN RESULT: The injectivity property described in the paper
'Language Models are Injective and Hence Invertible' is NOT limited to
decoder architectures. BERT encoders also exhibit this property.

THEORETICAL IMPLICATIONS:
  â€¢ Injectivity is a fundamental property of transformer architectures
  â€¢ Both causal and bidirectional attention preserve uniqueness
  â€¢ Input recovery should be possible for both architectures

PRACTICAL IMPLICATIONS:
  â€¢ Security concerns apply to all transformer-based models
  â€¢ BERT embeddings (widely used) may leak input information
  â€¢ Interpretability tools could recover inputs from embeddings
  â€¢ Privacy-preserving techniques needed for both architectures

FUTURE WORK:
  â€¢ Develop encoder-specific inversion algorithms
  â€¢ Test on larger models (BERT-large, RoBERTa, etc.)
  â€¢ Investigate pooling strategy effects on invertibility
  â€¢ Compare computational complexity of inversion

================================================================================
Report generated successfully.
================================================================================
