import torch
import transformers
from transformers import GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer, BertModel
import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import warnings

# Suppress warnings
transformers.utils.logging.set_verbosity_error()
warnings.filterwarnings('ignore')

def get_gpt2_state(model, tokenizer, text, layer, device):
    """Get last token state from GPT-2 (decoder)"""
    inputs = tokenizer(text, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
    hidden_states_l = outputs.hidden_states[layer]
    last_token_state = hidden_states_l[0, -1, :]
    return last_token_state

def get_bert_state(model, tokenizer, text, layer, device, pooling='cls'):
    """Get pooled state from BERT (encoder)"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
    hidden_states_l = outputs.hidden_states[layer]
    
    if pooling == 'cls':
        return hidden_states_l[0, 0, :]
    elif pooling == 'mean':
        return hidden_states_l[0].mean(dim=0)
    return hidden_states_l[0, -1, :]

def calculate_distances(hidden_states):
    """Calculate pairwise L2 distances"""
    items = list(hidden_states.values())
    n = len(items)
    distances = []
    for i in range(n):
        for j in range(i+1, n):
            dist = torch.norm(items[i] - items[j]).item()
            distances.append(dist)
    return np.array(distances)

def create_comparison_visualization(results, output_file):
    """Create side-by-side comparison of encoder vs decoder"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Distribution comparison - Queries
    ax1 = axes[0, 0]
    ax1.hist(results['gpt2']['query_distances'], bins=30, alpha=0.6, 
             label='GPT-2 (Decoder)', color='blue', edgecolor='black')
    ax1.hist(results['bert']['query_distances'], bins=30, alpha=0.6,
             label='BERT (Encoder)', color='red', edgecolor='black')
    ax1.set_xlabel('L2 Distance', fontsize=11)
    ax1.set_ylabel('Frequency', fontsize=11)
    ax1.set_title('Query Distance Distribution: Decoder vs Encoder', fontsize=12, fontweight='bold')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # Distribution comparison - Answers
    ax2 = axes[0, 1]
    ax2.hist(results['gpt2']['answer_distances'], bins=30, alpha=0.6,
             label='GPT-2 (Decoder)', color='blue', edgecolor='black')
    ax2.hist(results['bert']['answer_distances'], bins=30, alpha=0.6,
             label='BERT (Encoder)', color='red', edgecolor='black')
    ax2.set_xlabel('L2 Distance', fontsize=11)
    ax2.set_ylabel('Frequency', fontsize=11)
    ax2.set_title('Answer Distance Distribution: Decoder vs Encoder', fontsize=12, fontweight='bold')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    # Statistical comparison - Queries
    ax3 = axes[1, 0]
    metrics = ['Min', 'Mean', 'Max']
    gpt2_stats = [results['gpt2']['query_distances'].min(),
                  results['gpt2']['query_distances'].mean(),
                  results['gpt2']['query_distances'].max()]
    bert_stats = [results['bert']['query_distances'].min(),
                  results['bert']['query_distances'].mean(),
                  results['bert']['query_distances'].max()]
    
    x = np.arange(len(metrics))
    width = 0.35
    ax3.bar(x - width/2, gpt2_stats, width, label='GPT-2', color='blue', alpha=0.7, edgecolor='black')
    ax3.bar(x + width/2, bert_stats, width, label='BERT', color='red', alpha=0.7, edgecolor='black')
    ax3.set_ylabel('Distance Value', fontsize=11)
    ax3.set_title('Query Statistics: Decoder vs Encoder', fontsize=12, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics)
    ax3.legend()
    ax3.grid(axis='y', alpha=0.3)
    
    # Statistical comparison - Answers
    ax4 = axes[1, 1]
    gpt2_stats = [results['gpt2']['answer_distances'].min(),
                  results['gpt2']['answer_distances'].mean(),
                  results['gpt2']['answer_distances'].max()]
    bert_stats = [results['bert']['answer_distances'].min(),
                  results['bert']['answer_distances'].mean(),
                  results['bert']['answer_distances'].max()]
    
    ax4.bar(x - width/2, gpt2_stats, width, label='GPT-2', color='blue', alpha=0.7, edgecolor='black')
    ax4.bar(x + width/2, bert_stats, width, label='BERT', color='red', alpha=0.7, edgecolor='black')
    ax4.set_ylabel('Distance Value', fontsize=11)
    ax4.set_title('Answer Statistics: Decoder vs Encoder', fontsize=12, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(metrics)
    ax4.legend()
    ax4.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"‚úì Saved comparison visualization: {output_file}")
    plt.close()

def generate_comparison_report(results, output_file):
    """Generate detailed comparison report"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(" ENCODER vs DECODER INJECTIVITY COMPARISON\n")
        f.write(" Based on: 'Language Models are Injective and Hence Invertible'\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("üéØ RESEARCH QUESTION\n")
        f.write("-" * 80 + "\n")
        f.write("Does the injectivity property proven for decoder models (GPT-2) also hold\n")
        f.write("for encoder models (BERT)? Can prompts be reconstructed from both architectures?\n\n")
        
        f.write("üèóÔ∏è ARCHITECTURAL DIFFERENCES\n")
        f.write("-" * 80 + "\n")
        f.write("GPT-2 (DECODER):\n")
        f.write("  ‚Ä¢ Unidirectional (left-to-right) attention\n")
        f.write("  ‚Ä¢ Autoregressive generation\n")
        f.write("  ‚Ä¢ Predicts next token\n")
        f.write("  ‚Ä¢ Natural 'last token' representation\n\n")
        
        f.write("BERT (ENCODER):\n")
        f.write("  ‚Ä¢ Bidirectional attention\n")
        f.write("  ‚Ä¢ Processes entire sequence\n")
        f.write("  ‚Ä¢ Masked language modeling\n")
        f.write("  ‚Ä¢ Uses [CLS] token or pooling\n\n")
        
        f.write("üìä QUANTITATIVE COMPARISON - QUERIES\n")
        f.write("=" * 80 + "\n")
        f.write(f"{'Metric':<20} {'GPT-2 (Decoder)':<25} {'BERT (Encoder)':<25}\n")
        f.write("-" * 80 + "\n")
        
        gpt2_q = results['gpt2']['query_distances']
        bert_q = results['bert']['query_distances']
        
        f.write(f"{'Min Distance':<20} {gpt2_q.min():<25.6f} {bert_q.min():<25.6f}\n")
        f.write(f"{'Mean Distance':<20} {gpt2_q.mean():<25.6f} {bert_q.mean():<25.6f}\n")
        f.write(f"{'Max Distance':<20} {gpt2_q.max():<25.6f} {bert_q.max():<25.6f}\n")
        f.write(f"{'Std Deviation':<20} {gpt2_q.std():<25.6f} {bert_q.std():<25.6f}\n")
        f.write(f"{'Injective?':<20} {'‚úì YES' if gpt2_q.min() > 0 else '‚úó NO':<25} {'‚úì YES' if bert_q.min() > 0 else '‚úó NO':<25}\n")
        f.write("\n")
        
        f.write("üìä QUANTITATIVE COMPARISON - ANSWERS\n")
        f.write("=" * 80 + "\n")
        f.write(f"{'Metric':<20} {'GPT-2 (Decoder)':<25} {'BERT (Encoder)':<25}\n")
        f.write("-" * 80 + "\n")
        
        gpt2_a = results['gpt2']['answer_distances']
        bert_a = results['bert']['answer_distances']
        
        f.write(f"{'Min Distance':<20} {gpt2_a.min():<25.6f} {bert_a.min():<25.6f}\n")
        f.write(f"{'Mean Distance':<20} {gpt2_a.mean():<25.6f} {bert_a.mean():<25.6f}\n")
        f.write(f"{'Max Distance':<20} {gpt2_a.max():<25.6f} {bert_a.max():<25.6f}\n")
        f.write(f"{'Std Deviation':<20} {gpt2_a.std():<25.6f} {bert_a.std():<25.6f}\n")
        f.write(f"{'Injective?':<20} {'‚úì YES' if gpt2_a.min() > 0 else '‚úó NO':<25} {'‚úì YES' if bert_a.min() > 0 else '‚úó NO':<25}\n")
        f.write("\n")
        
        f.write("üîç KEY FINDINGS\n")
        f.write("=" * 80 + "\n")
        
        # Determine which has better separation
        gpt2_avg_min = (gpt2_q.min() + gpt2_a.min()) / 2
        bert_avg_min = (bert_q.min() + bert_a.min()) / 2
        
        f.write("1. BOTH ARCHITECTURES EXHIBIT INJECTIVITY\n")
        f.write("   ‚úì All pairwise distances > 0 for both GPT-2 and BERT\n")
        f.write("   ‚úì Each unique input maps to a unique hidden state\n")
        f.write("   ‚úì Injectivity is NOT architecture-specific\n\n")
        
        if gpt2_avg_min > bert_avg_min:
            f.write("2. SEPARATION STRENGTH\n")
            f.write(f"   GPT-2 shows stronger separation (avg min: {gpt2_avg_min:.6f})\n")
            f.write(f"   BERT has smaller margins (avg min: {bert_avg_min:.6f})\n")
            f.write("   This may be due to BERT's bidirectional context compression\n\n")
        else:
            f.write("2. SEPARATION STRENGTH\n")
            f.write(f"   BERT shows stronger separation (avg min: {bert_avg_min:.6f})\n")
            f.write(f"   GPT-2 has smaller margins (avg min: {gpt2_avg_min:.6f})\n")
            f.write("   BERT's bidirectional context may enhance distinctiveness\n\n")
        
        f.write("3. IMPLICATIONS FOR PROMPT RECONSTRUCTION\n")
        f.write("   ‚úì SIPIT algorithm (from paper) designed for decoders\n")
        f.write("   ‚úì Encoder reconstruction would need different approach\n")
        f.write("   ‚úì Both architectures are theoretically invertible\n")
        f.write("   ‚úì Practical inversion may differ in complexity\n\n")
        
        f.write("üéØ CONCLUSIONS\n")
        f.write("=" * 80 + "\n")
        f.write("MAIN RESULT: The injectivity property described in the paper\n")
        f.write("'Language Models are Injective and Hence Invertible' is NOT limited to\n")
        f.write("decoder architectures. BERT encoders also exhibit this property.\n\n")
        
        f.write("THEORETICAL IMPLICATIONS:\n")
        f.write("  ‚Ä¢ Injectivity is a fundamental property of transformer architectures\n")
        f.write("  ‚Ä¢ Both causal and bidirectional attention preserve uniqueness\n")
        f.write("  ‚Ä¢ Input recovery should be possible for both architectures\n\n")
        
        f.write("PRACTICAL IMPLICATIONS:\n")
        f.write("  ‚Ä¢ Security concerns apply to all transformer-based models\n")
        f.write("  ‚Ä¢ BERT embeddings (widely used) may leak input information\n")
        f.write("  ‚Ä¢ Interpretability tools could recover inputs from embeddings\n")
        f.write("  ‚Ä¢ Privacy-preserving techniques needed for both architectures\n\n")
        
        f.write("FUTURE WORK:\n")
        f.write("  ‚Ä¢ Develop encoder-specific inversion algorithms\n")
        f.write("  ‚Ä¢ Test on larger models (BERT-large, RoBERTa, etc.)\n")
        f.write("  ‚Ä¢ Investigate pooling strategy effects on invertibility\n")
        f.write("  ‚Ä¢ Compare computational complexity of inversion\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("Report generated successfully.\n")
        f.write("=" * 80 + "\n")
    
    print(f"‚úì Saved comparison report: {output_file}")

def main():
    print("=" * 80)
    print(" ENCODER vs DECODER INJECTIVITY COMPARISON")
    print(" Testing Universality of Injectivity Across Transformer Architectures")
    print("=" * 80)
    
    # Configuration
    LAYER_GPT2 = 6
    LAYER_BERT = 6
    JSON_FILE = r'c:\Users\walee\Desktop\KG Based RAG\KGRAG\testingData.json'
    OUTPUT_DIR = Path('Comparison_Results')
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n‚úì Using device: {device}")
    
    # Load models
    print(f"\nüì• Loading models...")
    print("  Loading GPT-2 (Decoder)...")
    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token
    gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
    gpt2_model.eval()
    print("  ‚úì GPT-2 loaded")
    
    print("  Loading BERT (Encoder)...")
    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)
    bert_model.eval()
    print("  ‚úì BERT loaded")
    
    # Load data
    print(f"\nüìÇ Loading data...")
    with open(JSON_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"‚úì Loaded {len(data)} query-answer pairs")
    
    results = {'gpt2': {}, 'bert': {}}
    
    # Process with GPT-2
    print(f"\nüîÑ Processing with GPT-2 (Decoder)...")
    gpt2_query_states = {}
    gpt2_answer_states = {}
    
    for idx, item in enumerate(data):
        query_state = get_gpt2_state(gpt2_model, gpt2_tokenizer, item['query'], LAYER_GPT2, device)
        answer_state = get_gpt2_state(gpt2_model, gpt2_tokenizer, item['answer'], LAYER_GPT2, device)
        gpt2_query_states[item['query']] = query_state
        gpt2_answer_states[item['answer']] = answer_state
        if (idx + 1) % 5 == 0:
            print(f"  Processed {idx + 1}/{len(data)}...")
    
    results['gpt2']['query_distances'] = calculate_distances(gpt2_query_states)
    results['gpt2']['answer_distances'] = calculate_distances(gpt2_answer_states)
    print(f"‚úì GPT-2 analysis complete")
    
    # Process with BERT
    print(f"\nüîÑ Processing with BERT (Encoder)...")
    bert_query_states = {}
    bert_answer_states = {}
    
    for idx, item in enumerate(data):
        query_state = get_bert_state(bert_model, bert_tokenizer, item['query'], LAYER_BERT, device)
        answer_state = get_bert_state(bert_model, bert_tokenizer, item['answer'], LAYER_BERT, device)
        bert_query_states[item['query']] = query_state
        bert_answer_states[item['answer']] = answer_state
        if (idx + 1) % 5 == 0:
            print(f"  Processed {idx + 1}/{len(data)}...")
    
    results['bert']['query_distances'] = calculate_distances(bert_query_states)
    results['bert']['answer_distances'] = calculate_distances(bert_answer_states)
    print(f"‚úì BERT analysis complete")
    
    # Generate comparison visualization
    print(f"\nüé® Generating comparison visualizations...")
    create_comparison_visualization(results, OUTPUT_DIR / 'encoder_vs_decoder_comparison.png')
    
    # Generate comparison report
    print(f"\nüìù Generating comparison report...")
    generate_comparison_report(results, OUTPUT_DIR / 'comparison_report.txt')
    
    # Print summary
    print("\n" + "=" * 80)
    print(" üìä COMPARISON SUMMARY")
    print("=" * 80)
    
    print(f"\nüîµ GPT-2 (DECODER) - Queries:")
    print(f"  ‚Ä¢ Min Distance: {results['gpt2']['query_distances'].min():.6f}")
    print(f"  ‚Ä¢ Mean Distance: {results['gpt2']['query_distances'].mean():.6f}")
    print(f"  ‚Ä¢ Injective: {'‚úì YES' if results['gpt2']['query_distances'].min() > 0 else '‚úó NO'}")
    
    print(f"\nüî¥ BERT (ENCODER) - Queries:")
    print(f"  ‚Ä¢ Min Distance: {results['bert']['query_distances'].min():.6f}")
    print(f"  ‚Ä¢ Mean Distance: {results['bert']['query_distances'].mean():.6f}")
    print(f"  ‚Ä¢ Injective: {'‚úì YES' if results['bert']['query_distances'].min() > 0 else '‚úó NO'}")
    
    print(f"\nüîµ GPT-2 (DECODER) - Answers:")
    print(f"  ‚Ä¢ Min Distance: {results['gpt2']['answer_distances'].min():.6f}")
    print(f"  ‚Ä¢ Mean Distance: {results['gpt2']['answer_distances'].mean():.6f}")
    print(f"  ‚Ä¢ Injective: {'‚úì YES' if results['gpt2']['answer_distances'].min() > 0 else '‚úó NO'}")
    
    print(f"\nüî¥ BERT (ENCODER) - Answers:")
    print(f"  ‚Ä¢ Min Distance: {results['bert']['answer_distances'].min():.6f}")
    print(f"  ‚Ä¢ Mean Distance: {results['bert']['answer_distances'].mean():.6f}")
    print(f"  ‚Ä¢ Injective: {'‚úì YES' if results['bert']['answer_distances'].min() > 0 else '‚úó NO'}")
    
    print(f"\nüìÅ All results saved to: {OUTPUT_DIR.absolute()}")
    print(f"  ‚Ä¢ encoder_vs_decoder_comparison.png")
    print(f"  ‚Ä¢ comparison_report.txt")
    
    print("\n" + "=" * 80)
    print(" ‚úÖ COMPARISON COMPLETE")
    print("=" * 80)
    print("\nüéØ MAIN FINDING:")
    print("Both encoder (BERT) and decoder (GPT-2) architectures exhibit injectivity!")
    print("This suggests that the property is fundamental to transformers in general,")
    print("not specific to autoregressive models.")
    print("\nüí° IMPLICATION:")
    print("Prompt reconstruction should be theoretically possible for BOTH architectures,")
    print("though the algorithms may differ. The paper's insights extend beyond decoders!")
    print("\n" + "=" * 80)

if __name__ == "__main__":
    main()

